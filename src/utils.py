import pandas as pd
import logging
import numpy as np
import unidecode
import re
from tqdm import tqdm
import json
from bs4 import *
import requests
from unicodedata import normalize


logging.basicConfig(level=logging.INFO)

ABSTAIN = -1
ORGANIZATION = 0
MAN = 1
WOMAN = 2
HUMAN = 3


def get_data(path, usecols=None, nrows=None):
    # Todo: is it convenient to drop duplicates?
    df = pd.read_csv(path, lineterminator='\n', header=0, na_values=[np.nan, "None ", "None"],
                     usecols=usecols, nrows=nrows).drop_duplicates('screen_name').reset_index(drop=True)
    df = df.fillna('')

    # This is only necessary for test sets:
    if 'label' not in df.columns:
        if 'User_1' in df.columns and 'User_2' in df.columns:
            arr1 = df['User_1'].to_numpy()
            arr2 = df['User_2'].to_numpy()
            if np.sum(arr1 != arr2) > 0:
                logging.info("Different annotations for test set!")
            df['label'] = df['User_1']
    if 'text' not in df.columns:
        df['text'] = df['tweet']
    if 'label' in df.columns:
        df['label'] = df.loc[:, 'label'].apply(lambda x: label_to_int(x))
        df = df.astype({"label": np.int8})
    return df


def label_to_int(x):
    """
    objective: get the labels in integer from string

    Inputs:
        - x, str: the string to consider
    Outputs:
        - int, 0 for orga, 1 for man, 2 for woman
    """
    if x in [ORGANIZATION, MAN, WOMAN, HUMAN, ABSTAIN]:
        return x
    if x == 'Man':
        return MAN
    elif x == 'Woman':
        return WOMAN
    elif x == 'Organization' or x == 'Institution':
        return ORGANIZATION
    elif x == 'Human':
        return HUMAN
    elif x == 'Unknown':
        return ABSTAIN
    else:
        raise Exception("Badly labeled user")


def multiple_replace(text, patterns, substitutions):
    # Create a regular expression  from the dictionary keys
    # regex = re.compile("(%r)" % "|".join(map(re.escape, d.keys())))
    pat = '|'.join(patterns.keys())

    # For each match, look-up corresponding value in dictionary
    return re.sub(pat, lambda mo: search_code(mo.string[mo.start():mo.end()], patterns, substitutions), text)


def preprocess_multiple_substitution(text):
    text = re.sub(r'[!\"#$%&\'()*+,-./:;<=>?@\[\\\]^_`{|}~]', ' ', text)
    return multiple_replace(text, patterns, substitutions)


def search_code(match, patterns, subs):
    for pat, code in patterns.items():
        if match in pat:
            return subs[code]
    logging.info(f"Could not correctly preprocess a character: {match}")
    return "No match"


def preprocess(x, rem_special_characters=True, rem_accents=True, lower=False, norm=False):
    if norm:
        x = normalize('NFKD', x)
    if lower:
        x = x.lower()
    if rem_accents:
        x = remove_accents(x)
    if rem_special_characters:
        x = strip_punctuation(x)
    return x


def find_ngrams(input_list, n):
    return list(zip(*[input_list[i:] for i in range(n)]))


def remove_accents_and_lower(text):
    """
    Objective: Lowercase and finally clear accents
    """
    text = unidecode.unidecode(text)
    text = text.lower()
    # Removes accents.
    cleaned_text = remove_accents(text)
    return cleaned_text


def strip_punctuation(text):
    """
    Objective: Preprocess data; substitute simbols by ' ' and finally clear accents
    """
    # cleaned_text = unidecode.unidecode(text)
    text = " ".join(
        re.findall("[a-zA-ZÃ€-Ã¿\u0621-\u064A\u0660-\u0669\u0E00-\u0E7F\uac00-\ud7a3\u3040-\u30ff\u4e00-\u9FFF]+", text))
    return text


def remove_accents(text):
    """
    Objective: Lowercase and finally clear accents
    """
    text = unidecode.unidecode(text)
    # Removes accents.
    text = re.sub(u"[Ã Ã¡Ã¢Ã£Ã¤Ã¥]", 'a', text)
    text = re.sub(u"[Ã¨Ã©ÃªÃ«]", 'e', text)
    text = re.sub(u"[Ã¬Ã­Ã®Ã¯]", 'i', text)
    text = re.sub(u"[Ã²Ã³Ã´ÃµÃ¶]", 'o', text)
    text = re.sub(u"[Ã¹ÃºÃ»Ã¼]", 'u', text)
    cleaned_text = re.sub(u"[Ã½Ã¿]", 'y', text)
    return cleaned_text


lc = {'a': u'[aÃ Ã¡Ã¢Ã£Ã¤Ã¥ÄƒÄ…ÇŽÇŸÇ¡Ç»ÈÈƒÈ§ðšð‘Žð’‚ð“ªð”žð•’ð–†ð–ºð—®ð˜¢ð™–ðšŠð°ðžª]',
      'b': u'[bð›ð‘ð“«ð”Ÿð•“ð–‡ð—¯ð˜£ð™—]',
      'c': u'[cÃ§Ä‡Ä‰Ä‹ÄÆˆðœð‘ð“¬ð•”ð–ˆð–¼ð—°ð˜¤ð™˜]',
      'd': u'[dÄÄ‘È¡ðð‘‘ð“­ð”¡ð••ð–‰ð˜¥ð™™]',
      'e': u'[eÃ¨Ã©ÃªÃ«Ä›Ä•Ä—Ä™È…È‡È©É‡ðžð‘’ð“®ð”¢ð•–ð–Šð–¾ð—²ð˜¦ð™šð’†]',
      'f': u'[fÆ’ðŸð‘“ð“¯ð”£ð•—ð–‹ð–¿ð—³ð˜§]',
      'g': u'[gÄÄŸÄ¡Ä£Ç¥Ç§Çµð ð‘”ð“°ð”¤ð•˜ð–Œð—€ð—´]',
      'h': u'[hÄ¥ÈŸÄ§ð¡ð’‰ð“±ð•™ð–ð—ð—µ]',
      'i': u'[iÃ¬Ã­Ã®Ã¯Ä©Ä«Ä­Ä¯ÇÈ‰È‹É¨ð¢ð‘–ð’Šð“²ð•šð–Žð—‚ð—¶ð˜ªð™žðš’ð’¾ð”¦]',
      'j': u'[jÄµð£ð‘—ð“³ð•›ð–ð—ƒð—·]',
      'k': u'[kÄ·Ç©Æ™ð¤ð‘˜ð“´ð•œð–ð—„ð—¸]',
      'l': u'[lÄºÄ¼Ä¾Å€Å‚ÆšÈ´ð¥ð‘™ð“µð•ð–‘ð—…ð—¹]',
      'm': u'[mð¦ð‘šð“¶ð•žð–’ð—†ð—º]',
      'n': u'[nÃ±Å„Å†ÅˆÇ¹ÈµÉ²ð§ð‘›ð“·ð•Ÿð–“ð—‡ð’]',
      'o': u'[oÃ²Ã³Ã´ÃµÃ¶Ã¸ÅÅÅ‘Æ¡Ç’Ç«Ç­ÈÈÈ¯È±ð¨ð‘œð“¸ð• ð–”ð—ˆð—¼ð˜°ð™¤ðš˜ðœŠð’]',
      'p': u'[pÆ¥ð©ð‘ð“¹ð•¡ð–•ð—‰ð—½]',
      'q': u'[qðªð‘žð“ºð•¢ð––ð—Šð—¾]',
      'r': u'[rÅ•Å—Å™È‘È“Éð«ð‘Ÿð“»ð•£ð–—ð—‹ð—¿ð’“ð“‡]',
      's': u'[sÅ›ÅÅŸÅ¡È™]',
      't': u'[tÅ¥Å£Å§È›ð–™]',
      'u': u'[uÃ¹ÃºÃ»Ã¼Å©Å«Å­Å¯Å±Å³Æ°Ç”Ç–Ç˜ÇšÇœÈ•È—ð“¾ð–š]',
      'v': u'[vÊ‹]',
      'w': u'[wÅµ]',
      'x': u'[x]',
      'y': u'[yÃ½Å·Ã¿È³ÉÆ´]',
      'z': u'[zÅºÅ¼Å¾Æ¶]'}

uc = {
    'A': u'[AÃ€ÃÃ‚ÃƒÃ„Ã…Ä‚Ä„ÇÇžÇ ÇºÈ€È‚È¦ð€ð‘¨ð’œð“ð”¸ð•¬ð– ð—”ð˜ˆð˜¼ð™°ðš¨ð›¢ðœœð–ðžðŸˆ]',
    'B': u'[Bðð‘©ð“‘ð”¹ð•­ð–¡ð—•ð˜‰ð˜½ð™±]',
    'C': u'[CÃ‡Ä†ÄˆÄŠÄŒÆ‡ð‚ð‘ªð’žð“’ð•®ð–¢ð—–ð˜Šð˜¾ð™²á´„]',
    'D': u'[DÄŽÄÈ¡ðƒð‘«ð““ð”»ð•¯ð–£ð——ð˜‹ð˜¿ð™³]',
    'E': u'[EÃˆÃ‰ÃŠÃ‹ÄšÄ”Ä–Ä˜È„È†È¨É†Æð„ð‘¬ð“”ð•°ð–¤ð—˜ð˜Œð™€]',
    'F': u'[FÆ‘ð…ð‘­ð“•ð”½ð•±ð–¥ð—™ð˜ð™]',
    'G': u'[GÄœÄžÄ Ä¢Ç¤Ç¦Ç´ð†ð‘®ð“–ð”¾ð•²ð–¦ð—šð˜Žð™‚]',
    'H': u'[HÄ¤ÈžÄ¦ð‡ð‘¯ð“—ð•³ð–§ð—›ð˜ð™ƒ]',
    'I': u'[IÃŒÃÃŽÃÄ¨ÄªÄ¬Ä®ÇÈˆÈŠÉªðˆð‘°ð“˜ð•´ð–¨ð—œð˜ð™„ðš°ð›ªðœ¤ðžðž˜]',
    'J': u'[JÄ´ð‰ð‘±ð“™ð•µð–©ð—ð˜‘ð™…]',
    'K': u'[KÄ¶Ç¨Æ˜ðŠð‘²ð“šð•¶ð–ªð—žð˜’ð™†]',
    'L': u'[LÄ¹Ä»Ä½Ä¿ÅÈ½ð‹ð‘³ð“›ð•·ð–«ð—Ÿð˜“ð™‡]',
    'M': u'[MðŒð‘´ð“œð•¸ð–¬ð— ð˜”ð™ˆ]',
    'N': u'[NÃ‘ÅƒÅ…Å‡Ç¸È É²ðð‘µð“ð•¹ð–­ð—¡ð˜•ð™‰]',
    'O': u'[OÃ’Ã“Ã”Ã•Ã–Ã˜ÅŒÅŽÅÆ Ç‘ÇªÇ¬ÈŒÈŽÈ®È°ðŽð‘¶ð“žð•ºð–®ð—¢ð˜–ð™Šðš¶ð›°ðœŠð¤ðžž]',
    'P': u'[PÆ¤ðð‘·ð“Ÿð•»ð–¯ð—£ð˜—ð™‹]',
    'Q': u'[Qðð‘¸ð“ ð•¼ð–°ð—¤ð˜˜ð™Œ]',
    'R': u'[RRÅ”Å–Å˜ÈÈ’ÉŒ]',
    'S': u'[SÅšÅœÅžÅ È˜Æ§]',
    'T': u'[TÅ¤Å¢Å¦ÈšÈ¾ð“ð‘»ð“£ð•‹ð–³ð—§ð˜›ð™ð“½]',
    'U': u'[UÃ™ÃšÃ›ÃœÅ¨ÅªÅ¬Å®Å°Å²Æ¯Ç“Ç•Ç—Ç™Ç›È”È–á´œ]',
    'V': u'[VÊ‹]',
    'W': u'[WÅ´]',
    'X': u'[X]',
    'Y': u'[YÃÅ¶Å¸È²ÉŽÆ³]',
    'Z': u'[ZÅ¹Å»Å½Æµ]'}

patterns = {val: num for num, val in enumerate(list(lc.values()) + list(uc.values()))}
substitutions = {num: val for num, val in enumerate(list(lc.keys()) + list(uc.keys()))}


def abc_dict(string_keys_dic):
    """
    Objective: From a string keys dictionary, create a nested dictionary, where First level keys is the abecedary,
    second level keys are the original keys, abecedary sorted.

    @param string_keys_dic: (dict) A dictionary whose keys are strings
    @return: (dict) A nested dictionary. First level keys is the abecedary, second level keys are the names
    """
    abc_dic = {}
    for key, value in string_keys_dic.items():
        if key[0] in abc_dic:
            abc_dic[key[0]][key] = value
        else:
            abc_dic[key[0]] = {key: value}
    return abc_dic

################################################## No need to have ####################################


def save_dict_as_json(path, dict):
    with open(path, 'w') as f:
        json.dump(dict, f)


def load_json_as_dict(path):
    try:
        f = open(path)
        data = f.read()
        return json.loads(data)
    except Exception as e:
        logging.info("ERROR : " + str(e))
    return False

# Spanish
def compute_dic_names_es(path_names, min_ngrams=4, min_rep=100):
    path_names += '/es/es_names.csv'
    names_es = pd.read_csv(path_names, sep=';', converters={'nombre': str})
    names_es.loc[:, 'nombre'] = names_es.loc[:, 'nombre'].apply(lambda x: int(x.replace('.', '')))
    names_es.loc[:,'preusuel'] = names_es.loc[:,'preusuel'].apply(lambda x: clean_text(x) if type(x) == str else x)
    names_es = names_es.loc[names_es.loc[:, 'preusuel'].apply(lambda x: len(x) if type(x) == str else 0)>1]
    return name_values(names_es, min_ngrams, min_rep)


def compute_dic_hypocoristic_names_es(path_names):
    names_v = load_json_as_dict(path_names)
    if names_v:
        return names_v
    url = "https://es.wikipedia.org/wiki/Anexo:Hipocor%C3%ADsticos_en_espa%C3%B1ol"
    r = requests.get(url)
    soup = BeautifulSoup(r.text, 'html.parser')
    names_v = {}
    for tag in soup.find_all("li"):
        if tag.i != None:
            hyp = tag.i.find(text=True, recursive=False)
            hyp = clean_text(hyp).split()
            n = tag.find(text=True)
            if '/' in n:
                n = n.split('/')
            else:
                n = [n]
            for n2 in n:
                n2 = clean_text(n2)
                names_v[n2] = hyp
    save_dict_as_json(path_names, names_v)
    return names_v


# French
def compute_dic_names_fr(path_names, min_ngrams=4, min_rep=100):
    path_names += '/fr/nat2020.csv'
    names_fr = pd.read_csv(path_names, sep=';')
    names_fr = names_fr.loc[names_fr.loc[:, "preusuel"] != "_PRENOMS_RARES"]
    names_fr.loc[:,'preusuel'] = names_fr.loc[:,'preusuel'].apply(lambda x: clean_text(x) if type(x) == str else x)
    names_fr = names_fr.loc[names_fr.loc[:, 'preusuel'].apply(lambda x: len(x) if type(x) == str else 0)>1]
    return name_values(names_fr, min_ngrams, min_rep)


def compute_dic_hypocoristic_names_fr(path_names, previous_dic):
    names_v = load_json_as_dict(path_names)
    if not names_v:
        url = "https://fr.wikipedia.org/wiki/Liste_de_pr%C3%A9noms_en_fran%C3%A7ais"
        r = requests.get(url)
        soup = BeautifulSoup(r.text, 'html.parser')
        names_v = {}
        mf = {'f': -1, 'm': 1}
        for tag in soup.find_all("li"):
            n = tag.text
            if ':' in n and ': origine et signification' not in n and ("(m)" in n or "(f)" in n):
                n = re.sub(r'\sou\s', ' ', n)
                names_info = clean_text(n.split(':')[0]).split()
                gender = names_info[0]
                ns = names_info[1:]
                for n in ns:
                    names_v[n] = mf[gender]
        save_dict_as_json(path_names, names_v)
    names_v = {key: value for key, value in names_v.items() if key not in previous_dic}
    return names_v

# English
def compute_dic_names_en(path_names, min_ngrams=4, min_rep=100):

    names_uk_w = pd.read_csv(path_names + '/en/f_names_uk.csv', sep=';', converters={'Count3': str})
    names_uk_m = pd.read_csv(path_names + '/en/m_names_uk.csv', sep=';', converters={'Count3': str})
    names_us = pd.read_csv(path_names + '/en/us_names.csv', sep=';', converters={'Count3': str})

    # For english language, we need to combine those from uk and us
    names_uk_m.loc[:, 'Count3'] = names_uk_m.loc[:, 'Count3'].apply(
        lambda x: int(x.replace(' ', '').replace('.', '')))
    names_uk_m.loc[:, 'Name'] = names_uk_m.loc[:, 'Name'].apply(lambda x: x[:-1] if x[-1] == ' ' else x)
    names_uk_w.loc[:, 'Count3'] = names_uk_w.loc[:, 'Count3'].apply(
        lambda x: int(x.replace(' ', '').replace('.', '')))
    names_uk_w.loc[:, 'Name'] = names_uk_w.loc[:, 'Name'].apply(lambda x: x[:-1] if x[-1] == ' ' else x)
    names_uk_w.loc[:, 'sexe'] = 2
    names_uk_m.loc[:, 'sexe'] = 1
    names_uk = pd.concat((names_uk_m.loc[:, ['Name', 'Count3', 'sexe']],
                          names_uk_w.loc[:, ['Name', 'Count3', 'sexe']]))
    names_uk.columns = ['preusuel', 'nombre', 'sexe']
    names_us.loc[:, 'sexe'] = names_us.loc[:, 'Gender'].apply(lambda x: 2 if x == 'F' else 1)
    names_us.loc[:, 'Name'] = names_us.loc[:, 'Name'].apply(lambda x: x.upper())
    names_us = names_us.rename(columns={'Frequency': 'nombre', 'Name': 'preusuel'})
    names_en = pd.concat((names_uk, names_us.loc[:, ['preusuel', 'nombre', 'sexe']]))
    names_en.loc[:,'preusuel'] = names_en.loc[:,'preusuel'].apply(lambda x: clean_text(x) if type(x) == str else x)
    names_en = names_en.loc[names_en.loc[:, 'preusuel'].apply(lambda x: len(x) if type(x) == str else 0)>1]
    return name_values(names_en, min_ngrams, min_rep)


def compute_dic_hypocoristic_names_en(path_names):
    names_v = load_json_as_dict(path_names)
    if names_v:
        return names_v
    url = "https://en.wiktionary.org/wiki/Appendix:English_given_names#A"
    r = requests.get(url)
    soup = BeautifulSoup(r.text, 'html.parser')
    names_v = {}
    for tag in soup.find_all("li"):
        h = tag.find(text=True, recursive=False)
        if tag.a != None and 'title' in tag.a.attrs and h:
            name = tag.a.find(text=True)
            name = clean_text(name)
            short_names = re.sub(r'\sor\s', ' ', h)
            short_names = clean_text(short_names)
            names_v[name] = short_names.split()
    save_dict_as_json(path_names, names_v)
    return names_v

#########################################################################################################
def name_values(names, min_ngrams, min_rep):
    gb = names.groupby(["preusuel", "sexe"])['nombre'].sum()
    man_woman_d = [{}, {}]
    for key, val in tqdm(gb.to_dict().items()):
        name, gender = key
        if val<min_rep and (len(name)<=min_ngrams or len(name.split())>1):
            continue
        man_woman_d[gender-1][name] = val
    d = name_values_from_dicts(man_woman_d[0], man_woman_d[1])
    return d


def relative_numbers(arr_1, arr_2, prior=0):
    """
    Objective: Given two lists of positive integers of same length, get the relative arr_1 to the sum of both lists

    @param arr_1: (array) Names as keys and incidence as values
    @param arr_2: (array) Names as keys and incidence as values
    @param prior: (int) Prior incidence
    @return p: (array) Relative numbers of arr_1 to the sum of both lists
    """

    n, m = len(arr_1), len(arr_2)
    if n != m:
        logging.info("Lists must have the same length")
        return []
    p1 = np.divide(prior + arr_1, 2*prior + arr_1 + arr_2)
    return p1

def name_values_from_dicts(dic_1, dic_2):
    """
    Objective: Build relative numbers of dic_1 compared to the sum of both dictionary values.

    @param dic_1: (dict) Containing names and the absolute numbers
    @param dic_2: (dict) Containing names and the absolute numbers
    @return p_dict: (dict) Relative numbers of dic_1 compared to the sum of both dictionary values
    """

    k1, k2 = dic_1.keys(), dic_2.keys()
    keys = list(set(k1).union(set(k2)))
    keys_length = len(keys)
    arr_1, arr_2 = np.zeros(keys_length), np.zeros(keys_length)
    it = range(keys_length)
    if keys_length > 1e8:
        it = tqdm(it)
    for i in it:
        key = keys[i]
        if key in k1:
            arr_1[i] = dic_1[key]
        if key in k2:
            arr_2[i] = dic_2[key]
    p1 = relative_numbers(arr_1, arr_2)
    p1_dict = dict(zip(keys, p1*2 - 1))
    return p1_dict